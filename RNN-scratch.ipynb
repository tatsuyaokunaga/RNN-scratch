{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スクラッチによるSimpleRNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(forward, backvalue):\n",
    "\n",
    "#params forward: forward function\n",
    "#params backward: gradient function\n",
    "\n",
    "    epsilon = 0.0001\n",
    "    gradient_checker = (forward(x + epsilon) -forward(x - epsilon)) / (2 * epsilon)\n",
    "    diff = np.abs(backvalue - gradient_checker)\n",
    "\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiplyGate:\n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "    def forward(self,W, x):\n",
    "        return np.dot(W, x)\n",
    "\n",
    "    def backward(self, W, x, dz):\n",
    "        dW = np.asarray(np.dot(dz, x.T))\n",
    "        dx = np.dot(np.transpose(W), dz)\n",
    "        return dW, dx\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "\n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz * np.ones_like(x1)\n",
    "        dx2 = dz * np.ones_like(x2)\n",
    "        return dx1, dx2\n",
    "\n",
    "class AdaGate:\n",
    "    def forward(self,x1, x2):\n",
    "        return x1 * x2\n",
    "    \n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz * x2\n",
    "        dx2 = dz * x1\n",
    "        return dx1, dx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def predict(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores,axis=0)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[y])\n",
    "\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[y] -= 1.0\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - np.square(output)) * top_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class L:\n",
    "    def forward(self, x, prev_s, U, W, V):\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.s = activation.forward(self.add)  \n",
    "        return self.add, self.s\n",
    "    \n",
    "    def backward(self, y, x, A, S, V, W, pred, num):\n",
    "        \n",
    "        diffs = pred - y\n",
    "        # dV\n",
    "        dV = np.dot(diffs , S[-1].T)\n",
    "        # dSt\n",
    "        pre_dSt = np.dot(V.T, diffs)\n",
    "        dU = 0\n",
    "        dW = 0\n",
    "        start = x.shape[1]-1\n",
    "        for i in range(start,start-num,-1):\n",
    "            #dAi\n",
    "            tanh = Tanh()\n",
    "            #print((1.0-np.square(A[i][5,6]))*pre_dSt[5,6]) 直接計算と、tanh backwardでは小数点以下８桁目あたりからずれることがある\n",
    "            dA = tanh.backward(A[i], pre_dSt)\n",
    "            #dU\n",
    "            dU += np.dot(dA, x[:,i,:])\n",
    "            #dW\n",
    "            dW += np.dot(dA, S[i-1].T)\n",
    "            # dSi-1\n",
    "            pre_dSt = np.dot(W.T, dA)      \n",
    "            \n",
    "        return dV, dU, dW        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, word_dim, hidden_dim=100, truncate=4, batchsize=32, optimizer = 'sgd'):\n",
    "        self.optimizer = optimizer\n",
    "        self.batchsize = batchsize\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.truncate = truncate\n",
    "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (2, hidden_dim))\n",
    "        if self.optimizer == 'adam':\n",
    "#             self.U = np.random.randn(hidden_dim, word_dim)\n",
    "#             self.W = np.random.randn(hidden_dim, hidden_dim)\n",
    "#             self.V = np.random.randn(2, hidden_dim)\n",
    "            self.m_U = np.zeros_like(self.U)\n",
    "            self.v_U = np.zeros_like(self.U)\n",
    "            self.m_W = np.zeros_like(self.W)\n",
    "            self.v_W = np.zeros_like(self.W)\n",
    "            self.m_V = np.zeros_like(self.V)\n",
    "            self.v_V = np.zeros_like(self.V)\n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.adam_lr = 0.001\n",
    "#         else:\n",
    "#             None\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward_propagation(self, x):   \n",
    "        T = x.shape[1]\n",
    "        self.all_A = []\n",
    "        self.all_S = []\n",
    "        prev_s = np.zeros((hidden_dim, x.shape[0]))\n",
    "        A = np.zeros((hidden_dim, x.shape[0]))\n",
    "        S = np.zeros((hidden_dim, x.shape[0]))\n",
    "        epsilon = 0.0001\n",
    "        dU_c = 0\n",
    "        dW_c = 0\n",
    "        dV_c = 0\n",
    "        for t in range(T):\n",
    "            layer = L()\n",
    "            A, S = layer.forward(x[:,t,:].T, prev_s, self.U, self.W, self.V)\n",
    "            \n",
    "#             if (T-self.truncate)<=t:\n",
    "#                 #grad_caheck\n",
    "#                  #dU\n",
    "#                 dU_c += (layer.forward(x[:,t,:].T, prev_s, self.U+epsilon, self.W, self.V) - \\\n",
    "#                         layer.forward(x[:,t,:].T, prev_s, self.U-epsilon, self.W, self.V)) / (2 * epsilon)\n",
    "#                 #dW\n",
    "#                 dW_c += (layer.forward(x[:,t,:].T, prev_s, self.U, self.W+epsilon, self.V) - \\\n",
    "#                         layer.forward(x[:,t,:].T, prev_s, self.U, self.W-epsilon, self.V)) / (2 * epsilon)\n",
    "            \n",
    "            prev_s = S\n",
    "            self.all_A.append(A)\n",
    "            self.all_S.append(S)     \n",
    "#         #dV\n",
    "#         dV_c = (layer.forward(x[:,T-1,:].T, prev_s, self.U, self.W, self.V+epsilon) - \\\n",
    "#                 layer.forward(x[:,T-1,:].T, prev_s, self.U, self.W, self.V-epsilon)) / (2 * epsilon)\n",
    "        #print(\"dU_c,dW_c,dV_c\",dU_c,dW_c,dV_c)\n",
    "        return np.dot(self.V, S)\n",
    "            \n",
    "    def loss_and_acc(self, x_train, y_train, x_test, y_test):\n",
    "        #forward\n",
    "        train_out = self.forward_propagation(x_train)\n",
    "        test_out = self.forward_propagation(x_test) \n",
    "        #predict\n",
    "        Soft = Softmax()\n",
    "        train_predict = Soft.predict(train_out)\n",
    "        test_predict = Soft.predict(test_out)\n",
    "        #acc\n",
    "        #0.５以上の方を採用する\n",
    "        train_predict2 = train_predict.copy()\n",
    "        test_predict2 = test_predict.copy()\n",
    "        train_predict2[train_predict2<0.5]=0\n",
    "        test_predict2[test_predict2<0.5]=0\n",
    "        train_predict2[0.5<=train_predict2]=1\n",
    "        test_predict2[0.5<=test_predict2]=1\n",
    "        self.train_acc = (train_predict2 * y_train).sum() / y_train.shape[1]\n",
    "        self.test_acc = (test_predict2 * y_test).sum() / y_test.shape[1]\n",
    "        #loss\n",
    "        train_log = train_predict * y_train # 正解側の確率だけ残す\n",
    "        self.train_loss = -np.log(train_log[np.where(1e-9<train_log)] +1e-8).sum() / y_train.shape[1]\n",
    "        test_log = test_predict * y_test # 正解側の確率だけ残す    \n",
    "        self.test_loss = -np.log(test_log[np.where(1e-9<test_log)] +1e-8).sum() / y_test.shape[1]\n",
    "    \n",
    "\n",
    "    def update(self, dV, dU, dW, learning_rate):\n",
    "        if self.optimizer == 'sgd':\n",
    "            self.U -= learning_rate * dU \n",
    "            self.V -= learning_rate * dV \n",
    "            self.W -= learning_rate * dW \n",
    "            \n",
    "        elif self.optimizer == 'adam':\n",
    "            \n",
    "            self.m_U = self.beta1 * self.m_U + (1- self.beta1) * dU\n",
    "            self.v_U = self.beta2 * self.v_U + (1- self.beta2) * (dU * dU)\n",
    "            m_hat_U = self.m_U / (1 - self.beta1)\n",
    "            v_hat_U = self.v_U / (1 - self.beta2)\n",
    "            self.U -= self.adam_lr * m_hat_U / (np.sqrt(v_hat_U) + 1e-8)\n",
    "            \n",
    "            self.m_V = self.beta1 * self.m_V + (1- self.beta1) * dV\n",
    "            self.v_V = self.beta2 * self.v_V + (1- self.beta2) * (dV * dV)\n",
    "            m_hat_V = self.m_V / (1 - self.beta1)\n",
    "            v_hat_V = self.v_V / (1 - self.beta2)\n",
    "            self.V -= self.adam_lr * m_hat_V / (np.sqrt(v_hat_V) + 1e-8)\n",
    "            \n",
    "            self.m_W = self.beta1 * self.m_W + (1- self.beta1) * dW\n",
    "            self.v_W = self.beta2 * self.v_W + (1- self.beta2) * (dW * dW)\n",
    "            m_hat_W = self.m_W / (1 - self.beta1)\n",
    "            v_hat_W = self.v_W / (1 - self.beta2)\n",
    "            self.W -= self.adam_lr * m_hat_W / (np.sqrt(v_hat_W) + 1e-8) \n",
    "            \n",
    "        else:\n",
    "            None          \n",
    "\n",
    "    def trains(self, Xtrain, Ytrain, Xtest, Ytest, learning_rate=0.005, nepoch=100):\n",
    "        iteration = len(Ytrain) // self.batchsize\n",
    "        y_hot = np.zeros((2,self.batchsize))  \n",
    "        pred1 = np.zeros((2,self.batchsize))\n",
    "        #Yrain ,Ytestをone-hot化しておく\n",
    "        Ytrain_hot = np.identity(2)[Ytrain].T\n",
    "        Ytest_hot = np.identity(2)[Ytest].T\n",
    "        Soft = Softmax()\n",
    "        #print(\"epoch\",nepoch)\n",
    "        #print(\"iteration\",iteration)\n",
    "        for epoch in range(nepoch):\n",
    "            for itr in range(iteration):\n",
    "                #print(itr)\n",
    "                start = itr*batchsize\n",
    "                x_batch = Xtrain[start:start+self.batchsize]\n",
    "                y_batch = Ytrain[start:start+self.batchsize]\n",
    "                y_hot = np.identity(2)[y_batch].T# (2, batchsize) 0行目が０ユニット用、1行目が１ユニット用\n",
    "                # forward\n",
    "                output = self.forward_propagation(x_batch)\n",
    "                #predict\n",
    "                pred1 = Soft.predict(output)\n",
    "                layers = L()\n",
    "                dV, dU, dW = layers.backward(y_hot, x_batch, self.all_A, self.all_S, self.V,  self.W, pred1, self.truncate)\n",
    "                \n",
    "                #update\n",
    "                self.update(dV, dU, dW, learning_rate)\n",
    "            \n",
    "            # loss\n",
    "            if (epoch==0) or ((epoch % 10) == 9):   \n",
    "                self.loss_and_acc(Xtrain, Ytrain_hot, Xtest, Ytest_hot)\n",
    "                print(\"*\"*50)\n",
    "                print(\"epoch\",epoch+1)\n",
    "                print(\"train : loss {:.4} ,acc {:.4}\".format(self.train_loss, self.train_acc))\n",
    "                print(\"test : loss {:.4} ,acc {:.4}\".format(self.test_loss, self.test_acc))\n",
    "                #print(\"output 0ユニット、１ユニット\",output[0,9],output[1,9])\n",
    "                #print(\"pred1 0ユニット、１ユニット\",pred1[0,9],pred1[1,9])\n",
    "                #print(self.V)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset\n",
      "padding\n",
      "build model\n",
      "**************************************************\n",
      "epoch 1\n",
      "train : loss 0.6511 ,acc 0.6209\n",
      "test : loss 0.6596 ,acc 0.6016\n",
      "**************************************************\n",
      "epoch 10\n",
      "train : loss 0.6059 ,acc 0.6683\n",
      "test : loss 0.6575 ,acc 0.6194\n",
      "**************************************************\n",
      "epoch 20\n",
      "train : loss 0.6026 ,acc 0.675\n",
      "test : loss 0.6633 ,acc 0.617\n",
      "**************************************************\n",
      "epoch 30\n",
      "train : loss 0.6029 ,acc 0.6745\n",
      "test : loss 0.6667 ,acc 0.6134\n",
      "**************************************************\n",
      "epoch 40\n",
      "train : loss 0.6039 ,acc 0.6737\n",
      "test : loss 0.6687 ,acc 0.6102\n",
      "**************************************************\n",
      "epoch 50\n",
      "train : loss 0.6047 ,acc 0.6729\n",
      "test : loss 0.6697 ,acc 0.6082\n",
      "**************************************************\n",
      "epoch 60\n",
      "train : loss 0.6053 ,acc 0.6731\n",
      "test : loss 0.6701 ,acc 0.6074\n",
      "**************************************************\n",
      "epoch 70\n",
      "train : loss 0.6056 ,acc 0.6723\n",
      "test : loss 0.6702 ,acc 0.6064\n",
      "**************************************************\n",
      "epoch 80\n",
      "train : loss 0.6058 ,acc 0.6721\n",
      "test : loss 0.6702 ,acc 0.6078\n",
      "**************************************************\n",
      "epoch 90\n",
      "train : loss 0.606 ,acc 0.6706\n",
      "test : loss 0.6702 ,acc 0.6088\n",
      "**************************************************\n",
      "epoch 100\n",
      "train : loss 0.6061 ,acc 0.6705\n",
      "test : loss 0.6703 ,acc 0.6082\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "\n",
    "word_dim = 128\n",
    "hidden_dim = 32\n",
    "bptt_truncate = 4\n",
    "batchsize = 16\n",
    "train_size =10000\n",
    "test_size=5000\n",
    "learning_rate=0.001\n",
    "nepoch=100\n",
    "np.random.seed(10) #シード固定\n",
    "\n",
    "# データ\n",
    "x_train, y_train, x_test, y_test=load_imdb(max_features = 10000, maxlen = 40)\n",
    "#モデル構築\n",
    "rnn = RNN(word_dim, hidden_dim ,bptt_truncate, batchsize, optimizer='adam')\n",
    "#訓練・検証実施\n",
    "rnn.trains(x_train[:train_size], y_train[:train_size],x_test[:test_size], y_test[:test_size], learning_rate, nepoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 検証データのaccuracyは約61%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
